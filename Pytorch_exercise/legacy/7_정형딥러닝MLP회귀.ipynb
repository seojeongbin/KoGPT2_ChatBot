{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_01</th>\n",
       "      <th>X_02</th>\n",
       "      <th>X_03</th>\n",
       "      <th>X_04</th>\n",
       "      <th>X_05</th>\n",
       "      <th>X_06</th>\n",
       "      <th>X_07</th>\n",
       "      <th>X_08</th>\n",
       "      <th>X_09</th>\n",
       "      <th>X_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Y_05</th>\n",
       "      <th>Y_06</th>\n",
       "      <th>Y_07</th>\n",
       "      <th>Y_08</th>\n",
       "      <th>Y_09</th>\n",
       "      <th>Y_10</th>\n",
       "      <th>Y_11</th>\n",
       "      <th>Y_12</th>\n",
       "      <th>Y_13</th>\n",
       "      <th>Y_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.544</td>\n",
       "      <td>103.320</td>\n",
       "      <td>67.47</td>\n",
       "      <td>1</td>\n",
       "      <td>101.892</td>\n",
       "      <td>74.983</td>\n",
       "      <td>29.45</td>\n",
       "      <td>62.38</td>\n",
       "      <td>245.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.632</td>\n",
       "      <td>16.083</td>\n",
       "      <td>4.276</td>\n",
       "      <td>-25.381</td>\n",
       "      <td>-25.529</td>\n",
       "      <td>-22.769</td>\n",
       "      <td>23.792</td>\n",
       "      <td>-25.470</td>\n",
       "      <td>-25.409</td>\n",
       "      <td>-25.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.524</td>\n",
       "      <td>103.321</td>\n",
       "      <td>65.17</td>\n",
       "      <td>1</td>\n",
       "      <td>101.944</td>\n",
       "      <td>72.943</td>\n",
       "      <td>28.73</td>\n",
       "      <td>61.23</td>\n",
       "      <td>233.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.179</td>\n",
       "      <td>16.736</td>\n",
       "      <td>3.229</td>\n",
       "      <td>-26.619</td>\n",
       "      <td>-26.523</td>\n",
       "      <td>-22.574</td>\n",
       "      <td>24.691</td>\n",
       "      <td>-26.253</td>\n",
       "      <td>-26.497</td>\n",
       "      <td>-26.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72.583</td>\n",
       "      <td>103.320</td>\n",
       "      <td>64.07</td>\n",
       "      <td>1</td>\n",
       "      <td>103.153</td>\n",
       "      <td>72.943</td>\n",
       "      <td>28.81</td>\n",
       "      <td>105.77</td>\n",
       "      <td>272.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.801</td>\n",
       "      <td>17.080</td>\n",
       "      <td>2.839</td>\n",
       "      <td>-26.238</td>\n",
       "      <td>-26.216</td>\n",
       "      <td>-22.169</td>\n",
       "      <td>24.649</td>\n",
       "      <td>-26.285</td>\n",
       "      <td>-26.215</td>\n",
       "      <td>-26.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71.563</td>\n",
       "      <td>103.320</td>\n",
       "      <td>67.57</td>\n",
       "      <td>1</td>\n",
       "      <td>101.971</td>\n",
       "      <td>77.022</td>\n",
       "      <td>28.92</td>\n",
       "      <td>115.21</td>\n",
       "      <td>255.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>34.503</td>\n",
       "      <td>17.143</td>\n",
       "      <td>3.144</td>\n",
       "      <td>-25.426</td>\n",
       "      <td>-25.079</td>\n",
       "      <td>-21.765</td>\n",
       "      <td>24.913</td>\n",
       "      <td>-25.254</td>\n",
       "      <td>-25.021</td>\n",
       "      <td>-25.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.524</td>\n",
       "      <td>103.320</td>\n",
       "      <td>63.57</td>\n",
       "      <td>1</td>\n",
       "      <td>101.981</td>\n",
       "      <td>70.904</td>\n",
       "      <td>29.68</td>\n",
       "      <td>103.38</td>\n",
       "      <td>241.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.602</td>\n",
       "      <td>17.569</td>\n",
       "      <td>3.138</td>\n",
       "      <td>-25.376</td>\n",
       "      <td>-25.242</td>\n",
       "      <td>-21.072</td>\n",
       "      <td>25.299</td>\n",
       "      <td>-25.072</td>\n",
       "      <td>-25.195</td>\n",
       "      <td>-24.974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39602</th>\n",
       "      <td>66.465</td>\n",
       "      <td>103.320</td>\n",
       "      <td>62.27</td>\n",
       "      <td>1</td>\n",
       "      <td>103.150</td>\n",
       "      <td>66.825</td>\n",
       "      <td>30.20</td>\n",
       "      <td>77.83</td>\n",
       "      <td>298.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.194</td>\n",
       "      <td>16.582</td>\n",
       "      <td>3.410</td>\n",
       "      <td>-26.486</td>\n",
       "      <td>-26.581</td>\n",
       "      <td>-22.772</td>\n",
       "      <td>24.261</td>\n",
       "      <td>-26.491</td>\n",
       "      <td>-26.584</td>\n",
       "      <td>-26.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39603</th>\n",
       "      <td>66.465</td>\n",
       "      <td>103.321</td>\n",
       "      <td>62.77</td>\n",
       "      <td>1</td>\n",
       "      <td>102.021</td>\n",
       "      <td>66.825</td>\n",
       "      <td>29.21</td>\n",
       "      <td>102.25</td>\n",
       "      <td>270.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.859</td>\n",
       "      <td>15.659</td>\n",
       "      <td>3.406</td>\n",
       "      <td>-27.308</td>\n",
       "      <td>-27.203</td>\n",
       "      <td>-24.674</td>\n",
       "      <td>23.427</td>\n",
       "      <td>-27.250</td>\n",
       "      <td>-27.334</td>\n",
       "      <td>-27.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39604</th>\n",
       "      <td>68.504</td>\n",
       "      <td>103.320</td>\n",
       "      <td>64.67</td>\n",
       "      <td>1</td>\n",
       "      <td>103.144</td>\n",
       "      <td>68.864</td>\n",
       "      <td>29.96</td>\n",
       "      <td>102.61</td>\n",
       "      <td>198.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.720</td>\n",
       "      <td>16.823</td>\n",
       "      <td>3.215</td>\n",
       "      <td>-26.502</td>\n",
       "      <td>-26.687</td>\n",
       "      <td>-22.577</td>\n",
       "      <td>24.301</td>\n",
       "      <td>-26.388</td>\n",
       "      <td>-26.425</td>\n",
       "      <td>-26.601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39605</th>\n",
       "      <td>66.465</td>\n",
       "      <td>103.320</td>\n",
       "      <td>63.67</td>\n",
       "      <td>1</td>\n",
       "      <td>102.025</td>\n",
       "      <td>67.845</td>\n",
       "      <td>30.30</td>\n",
       "      <td>112.60</td>\n",
       "      <td>275.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.412</td>\n",
       "      <td>15.757</td>\n",
       "      <td>4.216</td>\n",
       "      <td>-26.760</td>\n",
       "      <td>-26.634</td>\n",
       "      <td>-24.066</td>\n",
       "      <td>23.305</td>\n",
       "      <td>-26.536</td>\n",
       "      <td>-26.751</td>\n",
       "      <td>-26.635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39606</th>\n",
       "      <td>66.465</td>\n",
       "      <td>103.320</td>\n",
       "      <td>65.67</td>\n",
       "      <td>1</td>\n",
       "      <td>102.004</td>\n",
       "      <td>69.884</td>\n",
       "      <td>30.16</td>\n",
       "      <td>112.90</td>\n",
       "      <td>276.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30.745</td>\n",
       "      <td>16.781</td>\n",
       "      <td>3.307</td>\n",
       "      <td>-26.054</td>\n",
       "      <td>-26.251</td>\n",
       "      <td>-23.257</td>\n",
       "      <td>24.450</td>\n",
       "      <td>-26.224</td>\n",
       "      <td>-26.256</td>\n",
       "      <td>-26.093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39607 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X_01     X_02   X_03  X_04     X_05    X_06   X_07    X_08    X_09  \\\n",
       "0      70.544  103.320  67.47     1  101.892  74.983  29.45   62.38  245.71   \n",
       "1      69.524  103.321  65.17     1  101.944  72.943  28.73   61.23  233.61   \n",
       "2      72.583  103.320  64.07     1  103.153  72.943  28.81  105.77  272.20   \n",
       "3      71.563  103.320  67.57     1  101.971  77.022  28.92  115.21  255.36   \n",
       "4      69.524  103.320  63.57     1  101.981  70.904  29.68  103.38  241.46   \n",
       "...       ...      ...    ...   ...      ...     ...    ...     ...     ...   \n",
       "39602  66.465  103.320  62.27     1  103.150  66.825  30.20   77.83  298.05   \n",
       "39603  66.465  103.321  62.77     1  102.021  66.825  29.21  102.25  270.67   \n",
       "39604  68.504  103.320  64.67     1  103.144  68.864  29.96  102.61  198.07   \n",
       "39605  66.465  103.320  63.67     1  102.025  67.845  30.30  112.60  275.52   \n",
       "39606  66.465  103.320  65.67     1  102.004  69.884  30.16  112.90  276.06   \n",
       "\n",
       "       X_10  ...    Y_05    Y_06   Y_07    Y_08    Y_09    Y_10    Y_11  \\\n",
       "0       0.0  ...  29.632  16.083  4.276 -25.381 -25.529 -22.769  23.792   \n",
       "1       0.0  ...  33.179  16.736  3.229 -26.619 -26.523 -22.574  24.691   \n",
       "2       0.0  ...  31.801  17.080  2.839 -26.238 -26.216 -22.169  24.649   \n",
       "3       0.0  ...  34.503  17.143  3.144 -25.426 -25.079 -21.765  24.913   \n",
       "4       0.0  ...  32.602  17.569  3.138 -25.376 -25.242 -21.072  25.299   \n",
       "...     ...  ...     ...     ...    ...     ...     ...     ...     ...   \n",
       "39602   0.0  ...  29.194  16.582  3.410 -26.486 -26.581 -22.772  24.261   \n",
       "39603   0.0  ...  29.859  15.659  3.406 -27.308 -27.203 -24.674  23.427   \n",
       "39604   0.0  ...  24.720  16.823  3.215 -26.502 -26.687 -22.577  24.301   \n",
       "39605   0.0  ...  26.412  15.757  4.216 -26.760 -26.634 -24.066  23.305   \n",
       "39606   0.0  ...  30.745  16.781  3.307 -26.054 -26.251 -23.257  24.450   \n",
       "\n",
       "         Y_12    Y_13    Y_14  \n",
       "0     -25.470 -25.409 -25.304  \n",
       "1     -26.253 -26.497 -26.438  \n",
       "2     -26.285 -26.215 -26.370  \n",
       "3     -25.254 -25.021 -25.345  \n",
       "4     -25.072 -25.195 -24.974  \n",
       "...       ...     ...     ...  \n",
       "39602 -26.491 -26.584 -26.580  \n",
       "39603 -27.250 -27.334 -27.325  \n",
       "39604 -26.388 -26.425 -26.601  \n",
       "39605 -26.536 -26.751 -26.635  \n",
       "39606 -26.224 -26.256 -26.093  \n",
       "\n",
       "[39607 rows x 70 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r'C:\\Users\\SeoJeongBin\\Desktop\\Code\\Team_project_gawon\\data\\train.csv'\n",
    "df = pd.read_csv(path)\n",
    "df = df.iloc[:,1:]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. 이상한 값 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X_01    0\n",
       "X_02    0\n",
       "X_03    0\n",
       "X_04    0\n",
       "X_05    0\n",
       "       ..\n",
       "Y_10    0\n",
       "Y_11    0\n",
       "Y_12    0\n",
       "Y_13    0\n",
       "Y_14    0\n",
       "Length: 70, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_04\n",
      "X_23\n",
      "X_47\n",
      "X_48\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_01</th>\n",
       "      <th>X_02</th>\n",
       "      <th>X_03</th>\n",
       "      <th>X_05</th>\n",
       "      <th>X_06</th>\n",
       "      <th>X_07</th>\n",
       "      <th>X_08</th>\n",
       "      <th>X_09</th>\n",
       "      <th>X_10</th>\n",
       "      <th>X_11</th>\n",
       "      <th>...</th>\n",
       "      <th>Y_05</th>\n",
       "      <th>Y_06</th>\n",
       "      <th>Y_07</th>\n",
       "      <th>Y_08</th>\n",
       "      <th>Y_09</th>\n",
       "      <th>Y_10</th>\n",
       "      <th>Y_11</th>\n",
       "      <th>Y_12</th>\n",
       "      <th>Y_13</th>\n",
       "      <th>Y_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.544</td>\n",
       "      <td>103.320</td>\n",
       "      <td>67.47</td>\n",
       "      <td>101.892</td>\n",
       "      <td>74.983</td>\n",
       "      <td>29.45</td>\n",
       "      <td>62.38</td>\n",
       "      <td>245.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.632</td>\n",
       "      <td>16.083</td>\n",
       "      <td>4.276</td>\n",
       "      <td>-25.381</td>\n",
       "      <td>-25.529</td>\n",
       "      <td>-22.769</td>\n",
       "      <td>23.792</td>\n",
       "      <td>-25.470</td>\n",
       "      <td>-25.409</td>\n",
       "      <td>-25.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.524</td>\n",
       "      <td>103.321</td>\n",
       "      <td>65.17</td>\n",
       "      <td>101.944</td>\n",
       "      <td>72.943</td>\n",
       "      <td>28.73</td>\n",
       "      <td>61.23</td>\n",
       "      <td>233.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.179</td>\n",
       "      <td>16.736</td>\n",
       "      <td>3.229</td>\n",
       "      <td>-26.619</td>\n",
       "      <td>-26.523</td>\n",
       "      <td>-22.574</td>\n",
       "      <td>24.691</td>\n",
       "      <td>-26.253</td>\n",
       "      <td>-26.497</td>\n",
       "      <td>-26.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72.583</td>\n",
       "      <td>103.320</td>\n",
       "      <td>64.07</td>\n",
       "      <td>103.153</td>\n",
       "      <td>72.943</td>\n",
       "      <td>28.81</td>\n",
       "      <td>105.77</td>\n",
       "      <td>272.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.801</td>\n",
       "      <td>17.080</td>\n",
       "      <td>2.839</td>\n",
       "      <td>-26.238</td>\n",
       "      <td>-26.216</td>\n",
       "      <td>-22.169</td>\n",
       "      <td>24.649</td>\n",
       "      <td>-26.285</td>\n",
       "      <td>-26.215</td>\n",
       "      <td>-26.370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     X_01     X_02   X_03     X_05    X_06   X_07    X_08    X_09  X_10  X_11  \\\n",
       "0  70.544  103.320  67.47  101.892  74.983  29.45   62.38  245.71   0.0   0.0   \n",
       "1  69.524  103.321  65.17  101.944  72.943  28.73   61.23  233.61   0.0   0.0   \n",
       "2  72.583  103.320  64.07  103.153  72.943  28.81  105.77  272.20   0.0   0.0   \n",
       "\n",
       "   ...    Y_05    Y_06   Y_07    Y_08    Y_09    Y_10    Y_11    Y_12    Y_13  \\\n",
       "0  ...  29.632  16.083  4.276 -25.381 -25.529 -22.769  23.792 -25.470 -25.409   \n",
       "1  ...  33.179  16.736  3.229 -26.619 -26.523 -22.574  24.691 -26.253 -26.497   \n",
       "2  ...  31.801  17.080  2.839 -26.238 -26.216 -22.169  24.649 -26.285 -26.215   \n",
       "\n",
       "     Y_14  \n",
       "0 -25.304  \n",
       "1 -26.438  \n",
       "2 -26.370  \n",
       "\n",
       "[3 rows x 66 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_columns = df.columns[:56]\n",
    "for col in x_columns:\n",
    "    if len(df[col].unique()) == 1:\n",
    "        print(col)\n",
    "        df = df.drop([col], axis=1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    39575\n",
      "2.9       12\n",
      "3.0       12\n",
      "3.3        4\n",
      "3.1        2\n",
      "3.6        1\n",
      "3.2        1\n",
      "Name: X_10, dtype: int64\n",
      "0.0    39580\n",
      "0.5       12\n",
      "0.6       11\n",
      "0.4        3\n",
      "0.7        1\n",
      "Name: X_11, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['X_10'].value_counts())\n",
    "print(df['X_11'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['X_10','X_11','X_46'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2. 이상치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR 기반 이상치 제거\n",
    "# dl 은 데이터 많으면 이상치 제거안해도 알아서 그거까지 잘 학습하는 식인듯."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 학습 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:, :-14]\n",
    "y = df.iloc[:, -14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_01</th>\n",
       "      <th>X_02</th>\n",
       "      <th>X_03</th>\n",
       "      <th>X_05</th>\n",
       "      <th>X_06</th>\n",
       "      <th>X_07</th>\n",
       "      <th>X_08</th>\n",
       "      <th>X_09</th>\n",
       "      <th>X_12</th>\n",
       "      <th>X_13</th>\n",
       "      <th>...</th>\n",
       "      <th>X_44</th>\n",
       "      <th>X_45</th>\n",
       "      <th>X_49</th>\n",
       "      <th>X_50</th>\n",
       "      <th>X_51</th>\n",
       "      <th>X_52</th>\n",
       "      <th>X_53</th>\n",
       "      <th>X_54</th>\n",
       "      <th>X_55</th>\n",
       "      <th>X_56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37287</th>\n",
       "      <td>70.544</td>\n",
       "      <td>103.320</td>\n",
       "      <td>65.17</td>\n",
       "      <td>103.150</td>\n",
       "      <td>70.904</td>\n",
       "      <td>31.77</td>\n",
       "      <td>222.29</td>\n",
       "      <td>270.78</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>21.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>14210.43</td>\n",
       "      <td>132.984753</td>\n",
       "      <td>132.917286</td>\n",
       "      <td>140.042024</td>\n",
       "      <td>130.635954</td>\n",
       "      <td>126.664081</td>\n",
       "      <td>134.437619</td>\n",
       "      <td>131.312313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2173</th>\n",
       "      <td>72.583</td>\n",
       "      <td>103.320</td>\n",
       "      <td>67.57</td>\n",
       "      <td>103.151</td>\n",
       "      <td>72.943</td>\n",
       "      <td>30.46</td>\n",
       "      <td>72.39</td>\n",
       "      <td>251.76</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>21.22</td>\n",
       "      <td>0.11</td>\n",
       "      <td>14159.23</td>\n",
       "      <td>137.039670</td>\n",
       "      <td>131.469418</td>\n",
       "      <td>145.135217</td>\n",
       "      <td>131.541037</td>\n",
       "      <td>131.482426</td>\n",
       "      <td>129.437153</td>\n",
       "      <td>135.319843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3609</th>\n",
       "      <td>72.583</td>\n",
       "      <td>103.321</td>\n",
       "      <td>74.27</td>\n",
       "      <td>101.947</td>\n",
       "      <td>72.943</td>\n",
       "      <td>33.77</td>\n",
       "      <td>1381.34</td>\n",
       "      <td>37.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>21.14</td>\n",
       "      <td>0.24</td>\n",
       "      <td>23228.33</td>\n",
       "      <td>122.577565</td>\n",
       "      <td>121.785160</td>\n",
       "      <td>125.584215</td>\n",
       "      <td>115.501127</td>\n",
       "      <td>126.467319</td>\n",
       "      <td>127.597096</td>\n",
       "      <td>125.188778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3027</th>\n",
       "      <td>66.465</td>\n",
       "      <td>103.320</td>\n",
       "      <td>68.57</td>\n",
       "      <td>102.005</td>\n",
       "      <td>66.825</td>\n",
       "      <td>25.59</td>\n",
       "      <td>109.78</td>\n",
       "      <td>179.77</td>\n",
       "      <td>4.37</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>21.11</td>\n",
       "      <td>0.19</td>\n",
       "      <td>11903.93</td>\n",
       "      <td>121.535229</td>\n",
       "      <td>128.199551</td>\n",
       "      <td>127.536043</td>\n",
       "      <td>124.847639</td>\n",
       "      <td>129.571697</td>\n",
       "      <td>132.293324</td>\n",
       "      <td>130.435955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31350</th>\n",
       "      <td>64.425</td>\n",
       "      <td>103.320</td>\n",
       "      <td>77.57</td>\n",
       "      <td>102.045</td>\n",
       "      <td>66.825</td>\n",
       "      <td>26.73</td>\n",
       "      <td>74.46</td>\n",
       "      <td>255.90</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>21.10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>16919.83</td>\n",
       "      <td>151.019604</td>\n",
       "      <td>148.448589</td>\n",
       "      <td>136.757616</td>\n",
       "      <td>135.160307</td>\n",
       "      <td>131.352207</td>\n",
       "      <td>138.953572</td>\n",
       "      <td>130.342182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>69.524</td>\n",
       "      <td>103.320</td>\n",
       "      <td>71.97</td>\n",
       "      <td>101.870</td>\n",
       "      <td>70.904</td>\n",
       "      <td>48.24</td>\n",
       "      <td>120.52</td>\n",
       "      <td>608.69</td>\n",
       "      <td>4.34</td>\n",
       "      <td>0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>21.09</td>\n",
       "      <td>0.19</td>\n",
       "      <td>15585.73</td>\n",
       "      <td>124.925084</td>\n",
       "      <td>124.813091</td>\n",
       "      <td>123.314043</td>\n",
       "      <td>116.561773</td>\n",
       "      <td>119.467594</td>\n",
       "      <td>122.416775</td>\n",
       "      <td>123.261044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>68.504</td>\n",
       "      <td>103.320</td>\n",
       "      <td>71.67</td>\n",
       "      <td>101.935</td>\n",
       "      <td>72.943</td>\n",
       "      <td>30.90</td>\n",
       "      <td>119.81</td>\n",
       "      <td>268.32</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>21.08</td>\n",
       "      <td>0.13</td>\n",
       "      <td>14228.73</td>\n",
       "      <td>126.370267</td>\n",
       "      <td>130.973187</td>\n",
       "      <td>135.857515</td>\n",
       "      <td>133.020071</td>\n",
       "      <td>132.835186</td>\n",
       "      <td>130.027691</td>\n",
       "      <td>126.445255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38158</th>\n",
       "      <td>71.563</td>\n",
       "      <td>103.320</td>\n",
       "      <td>66.77</td>\n",
       "      <td>103.156</td>\n",
       "      <td>71.923</td>\n",
       "      <td>28.67</td>\n",
       "      <td>2362.16</td>\n",
       "      <td>37.58</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>21.20</td>\n",
       "      <td>0.13</td>\n",
       "      <td>12088.23</td>\n",
       "      <td>137.139912</td>\n",
       "      <td>134.100041</td>\n",
       "      <td>147.350031</td>\n",
       "      <td>130.241667</td>\n",
       "      <td>134.185695</td>\n",
       "      <td>147.163205</td>\n",
       "      <td>133.407581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>68.504</td>\n",
       "      <td>103.320</td>\n",
       "      <td>70.67</td>\n",
       "      <td>101.966</td>\n",
       "      <td>68.864</td>\n",
       "      <td>29.44</td>\n",
       "      <td>232.11</td>\n",
       "      <td>200.89</td>\n",
       "      <td>4.38</td>\n",
       "      <td>0.20</td>\n",
       "      <td>...</td>\n",
       "      <td>21.15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>19974.53</td>\n",
       "      <td>130.648595</td>\n",
       "      <td>127.940746</td>\n",
       "      <td>135.476869</td>\n",
       "      <td>124.291716</td>\n",
       "      <td>122.022510</td>\n",
       "      <td>128.166859</td>\n",
       "      <td>124.283352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>67.485</td>\n",
       "      <td>103.320</td>\n",
       "      <td>64.67</td>\n",
       "      <td>102.013</td>\n",
       "      <td>68.864</td>\n",
       "      <td>27.70</td>\n",
       "      <td>167.47</td>\n",
       "      <td>295.50</td>\n",
       "      <td>4.35</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>21.12</td>\n",
       "      <td>0.18</td>\n",
       "      <td>17411.03</td>\n",
       "      <td>129.951794</td>\n",
       "      <td>147.583119</td>\n",
       "      <td>143.455698</td>\n",
       "      <td>132.143484</td>\n",
       "      <td>130.631039</td>\n",
       "      <td>148.609951</td>\n",
       "      <td>127.784481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27724 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X_01     X_02   X_03     X_05    X_06   X_07     X_08    X_09  X_12  \\\n",
       "37287  70.544  103.320  65.17  103.150  70.904  31.77   222.29  270.78  4.38   \n",
       "2173   72.583  103.320  67.57  103.151  72.943  30.46    72.39  251.76  4.38   \n",
       "3609   72.583  103.321  74.27  101.947  72.943  33.77  1381.34   37.58  4.38   \n",
       "3027   66.465  103.320  68.57  102.005  66.825  25.59   109.78  179.77  4.37   \n",
       "31350  64.425  103.320  77.57  102.045  66.825  26.73    74.46  255.90  4.38   \n",
       "...       ...      ...    ...      ...     ...    ...      ...     ...   ...   \n",
       "6265   69.524  103.320  71.97  101.870  70.904  48.24   120.52  608.69  4.34   \n",
       "11284  68.504  103.320  71.67  101.935  72.943  30.90   119.81  268.32  4.38   \n",
       "38158  71.563  103.320  66.77  103.156  71.923  28.67  2362.16   37.58  4.38   \n",
       "860    68.504  103.320  70.67  101.966  68.864  29.44   232.11  200.89  4.38   \n",
       "15795  67.485  103.320  64.67  102.013  68.864  27.70   167.47  295.50  4.35   \n",
       "\n",
       "       X_13  ...   X_44  X_45      X_49        X_50        X_51        X_52  \\\n",
       "37287  0.15  ...  21.18  0.17  14210.43  132.984753  132.917286  140.042024   \n",
       "2173   0.13  ...  21.22  0.11  14159.23  137.039670  131.469418  145.135217   \n",
       "3609   0.14  ...  21.14  0.24  23228.33  122.577565  121.785160  125.584215   \n",
       "3027   0.17  ...  21.11  0.19  11903.93  121.535229  128.199551  127.536043   \n",
       "31350  0.14  ...  21.10  0.16  16919.83  151.019604  148.448589  136.757616   \n",
       "...     ...  ...    ...   ...       ...         ...         ...         ...   \n",
       "6265   0.15  ...  21.09  0.19  15585.73  124.925084  124.813091  123.314043   \n",
       "11284  0.09  ...  21.08  0.13  14228.73  126.370267  130.973187  135.857515   \n",
       "38158  0.14  ...  21.20  0.13  12088.23  137.139912  134.100041  147.350031   \n",
       "860    0.20  ...  21.15  0.20  19974.53  130.648595  127.940746  135.476869   \n",
       "15795  0.14  ...  21.12  0.18  17411.03  129.951794  147.583119  143.455698   \n",
       "\n",
       "             X_53        X_54        X_55        X_56  \n",
       "37287  130.635954  126.664081  134.437619  131.312313  \n",
       "2173   131.541037  131.482426  129.437153  135.319843  \n",
       "3609   115.501127  126.467319  127.597096  125.188778  \n",
       "3027   124.847639  129.571697  132.293324  130.435955  \n",
       "31350  135.160307  131.352207  138.953572  130.342182  \n",
       "...           ...         ...         ...         ...  \n",
       "6265   116.561773  119.467594  122.416775  123.261044  \n",
       "11284  133.020071  132.835186  130.027691  126.445255  \n",
       "38158  130.241667  134.185695  147.163205  133.407581  \n",
       "860    124.291716  122.022510  128.166859  124.283352  \n",
       "15795  132.143484  130.631039  148.609951  127.784481  \n",
       "\n",
       "[27724 rows x 49 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "x_train # df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 0.26605505, ..., 0.68520005, 0.76514467,\n",
       "        0.84000076],\n",
       "       [0.57141356, 0.        , 0.33944954, ..., 0.71654645, 0.73081701,\n",
       "        0.87063095],\n",
       "       [0.57141356, 1.        , 0.54434251, ..., 0.68391999, 0.71818521,\n",
       "        0.79319762],\n",
       "       ...,\n",
       "       [0.53568927, 0.        , 0.31498471, ..., 0.73413293, 0.85250446,\n",
       "        0.85601523],\n",
       "       [0.42855141, 0.        , 0.43425076, ..., 0.65500368, 0.72209657,\n",
       "        0.7862773 ],\n",
       "       [0.39286215, 0.        , 0.25076453, ..., 0.71100763, 0.86243622,\n",
       "        0.81303699]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler() # x,y 따로 나누는 이유는 혹시나 실제 y값을 print할때 찌부된거 복원시키기 위함\n",
    "\n",
    "x_train = x_scaler.fit_transform(x_train)\n",
    "x_test = x_scaler.transform(x_test)\n",
    "y_train = x_scaler.fit_transform(y_train)\n",
    "y_test = x_scaler.transform(y_test)\n",
    "\n",
    "x_train\n",
    "# ndarray (df아님)\n",
    "# ml이든 dl이든 학습돌릴때도 df가 아니라 값이여야함. df면 .values하거나 ndarray로 바꿔줘야"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. pytorch - Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    # 1) 전처리 (ndaaray => tensor)\n",
    "    def __init__(self, x, y) :\n",
    "        super().__init__()\n",
    "        # input이 df면 .values, ndarray면 생략\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    # 2) len() 했을때 개수반환 해주는 역할    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    # 3) dataset[i] 했을때 i번째 데이터 호출됨\n",
    "    def __getitem__(self, index) :\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "train_dataset = MyDataset(x_train, y_train)\n",
    "test_dataset = MyDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset 개수 : 27724개\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.5000, 0.0000, 0.2661, 0.9928, 0.3913, 0.0592, 0.0783, 0.3887, 0.5000,\n",
       "         0.4348, 0.6765, 0.5926, 0.6286, 0.6316, 0.6667, 0.3933, 0.4167, 0.2706,\n",
       "         0.3723, 0.3939, 0.3611, 0.3514, 0.3889, 0.4524, 0.5588, 0.5325, 0.1437,\n",
       "         0.5130, 0.1306, 0.2083, 0.4286, 0.5833, 0.4444, 0.1053, 0.1066, 0.5034,\n",
       "         0.5393, 0.4603, 0.7544, 0.6316, 0.4211, 0.0977, 0.8230, 0.7992, 0.8127,\n",
       "         0.8397, 0.6852, 0.7651, 0.8400]),\n",
       " tensor([0.3150, 0.2273, 0.2648, 0.4034, 0.6757, 0.9396, 0.5767, 0.5497, 0.5469,\n",
       "         0.8069, 0.6494, 0.4933, 0.5411, 0.5658]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'train_dataset 개수 : {len(train_dataset)}개')\n",
    "\n",
    "train_dataset[0] # (0번째 x train의 torch, 0번째 y train의 torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. pytorch - Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-1. 모델 클래스 짜기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class myMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, drop_ratio) :\n",
    "        super(myMLP, self).__init__()\n",
    "        # self.input_size = input_size => 이런거 다른 함수에서 사용하는 경우라면 self.로 받는거다\n",
    "        self.layer_mlp = nn.Sequential(\n",
    "            nn.Linear(input_size, 18),\n",
    "            torch.nn.BatchNorm1d(18), # 배치정규화\n",
    "            nn.Linear(18, 9),\n",
    "            torch.nn.BatchNorm1d(9),\n",
    "            nn.Linear(9, output_size),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.layer_dropout = torch.nn.Dropout(p=drop_ratio)\n",
    "        self.layer_softmax = torch.nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.layer_mlp(x)\n",
    "        output = self.layer_dropout(output)\n",
    "        output = self.layer_softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-2. Dataloader => test dataloader는 shuffle False 해야하는듯?\n",
    "- batch size 설정가능\n",
    "- iterable한 객체로 감쌀 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=768, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=768, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 49])\n",
      "torch.Size([768, 14])\n"
     ]
    }
   ],
   "source": [
    "loader = iter(train_dataloader)\n",
    "x, y = next(loader) # 첫번째 batch가 나오는거임 이렇게하면\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader) # 총 사이즈 / batch_size 인듯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3. 모델 객체 선언 & optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = iter(train_dataloader)\n",
    "x, y = next(loader) # 개수 입력해줄 용도로..\n",
    "\n",
    "input_size = x.shape[1] # 입력변수 개수(칼럼 개수)\n",
    "output_size = y.shape[1] # 예측변수 개수(칼럼 개수)\n",
    "drop_ratio = 0.8\n",
    "\n",
    "model = myMLP(input_size, output_size, drop_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]C:\\Users\\SeoJeongBin\\AppData\\Local\\Temp\\ipykernel_15956\\4194624756.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.layer_softmax(output)\n",
      "  0%|          | 1/500 [00:00<06:58,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 1/500] [train_loss : 0.2875534819590079] [val_loss : 0.2777359411120415]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 51/500 [00:32<04:49,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 51/500] [train_loss : 0.27525297934944565] [val_loss : 0.27527580223977566]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 101/500 [01:00<04:04,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 101/500] [train_loss : 0.2744537821492633] [val_loss : 0.2753304336220026]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 151/500 [01:28<03:33,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 151/500] [train_loss : 0.27478573773358317] [val_loss : 0.2750057727098465]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 201/500 [01:56<02:53,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 201/500] [train_loss : 0.27522035949939005] [val_loss : 0.275224294513464]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 251/500 [02:23<02:23,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 251/500] [train_loss : 0.2748985717425475] [val_loss : 0.2751185651868582]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 301/500 [02:50<02:03,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 301/500] [train_loss : 0.2748947489906002] [val_loss : 0.27517043985426426]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 351/500 [03:17<01:28,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 351/500] [train_loss : 0.27493114970825816] [val_loss : 0.2752594165503979]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 401/500 [03:43<00:56,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 401/500] [train_loss : 0.2755530323531177] [val_loss : 0.27519855089485645]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 451/500 [04:09<00:27,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 451/500] [train_loss : 0.27510401526012934] [val_loss : 0.27530554309487343]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [04:35<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "epoch_num = 500\n",
    "\n",
    "for epoch in tqdm(range(epoch_num)):\n",
    "    # train mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for x, y in train_dataloader :\n",
    "        # x,y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item() # 여기서 data loader 길이만큼 += 되니까 출력으로 볼때는 길이로 나눠줘야 함 당연히!\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 50 == 0 : # 100 배수의 epoch 마다 학습된 파라미터로 테스트 셋 평가해봄 \n",
    "        # eval mode\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad() : # test set은 학습없으니 역전파 없다\n",
    "            for x,y in test_dataloader:\n",
    "                # x,y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "                pred = model(x)\n",
    "                loss = loss_fn(pred, y)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        print(f'[Epoch : {epoch+1}/{epoch_num}] [train_loss : {train_loss/len(train_dataloader)}] [val_loss : {val_loss/len(test_dataloader)}]')\n",
    "        # loss 길이로 꼭 나눠서 평균으로 봐야함!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kotorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e394b0e5950b2ed9ac02d3d0f43da50df004fa03f183c9ffe2862d4a06a95d21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
