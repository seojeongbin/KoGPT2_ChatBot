{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lightning](./lightning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그냥 싹다 class 내부 def 함수로 정의하고 마지막에 모델 객체 생성 후 fit하면 모든걸 다 해결해주는 느낌**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기존 pytorch는 DataLoader, Model, optimizer, Training roof 등을 전부 따로따로 코드 구현\n",
    "- Lightning : Model class 안에 이 모든 것을 한 번에 구현\n",
    "    - Lightning Module : 모델 내부 구조 설계\n",
    "        - 구조, 전처리, 손실함수 등 정의\n",
    "    - Trainer : 모델 학습 관련 (위 그림에서 파란색 부분!)\n",
    "        - epoch, batch, 저장, 로그 관련 등\n",
    "        - 코드로는 pl.Trainer()로 하면 끝 : 특히 두줄로 많이 줄었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch import Tensor, nn\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning Module은 6가지로 구성됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self): # 초기화 메서드\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            ...\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass # 모델의 추론결과 : f(x)식에 입력된 x로부터 예측된 y를 얻는 것을 forward 연산이라 함\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pass # 단일 배치에서의 loss 반환 <- train loop으로 자동 반복\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        pass # 최적 파라미터를 찾을 때 사용할 optimizer, schduler 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Boston House' example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        super().__init__()\n",
    "        scaler = MinMaxScaler() \n",
    "\n",
    "        scaler.fit(X) \n",
    "        self.X = scaler.transform(X)\n",
    "        self.Y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx].astype(np.float32)\n",
    "        y = self.Y[idx].astype(np.float32)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)\n",
    "bostonds = SklearnDataset(X, y)\n",
    "train_loader = DataLoader(bostonds, batch_size=32, shuffle=True, drop_last=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinRegModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features=13, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_hat = self.linear(x)\n",
    "        return y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # flatten any input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y, reduction=\"sum\")\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\SeoJeongBin\\miniconda3\\envs\\kotorch\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:89: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: c:\\Users\\SeoJeongBin\\Desktop\\작업\\KoGPT2_ChatBot\\Pytorch_lightning\\lightning_logs\n",
      "\n",
      "  | Name   | Type   | Params\n",
      "----------------------------------\n",
      "0 | linear | Linear | 14    \n",
      "----------------------------------\n",
      "14        Trainable params\n",
      "0         Non-trainable params\n",
      "14        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "c:\\Users\\SeoJeongBin\\miniconda3\\envs\\kotorch\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\SeoJeongBin\\miniconda3\\envs\\kotorch\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1892: PossibleUserWarning: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  20%|██        | 3/15 [00:00<00:00, 27.52it/s, loss=5.58e+05, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SeoJeongBin\\AppData\\Local\\Temp\\ipykernel_2596\\1653378779.py:15: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(y_hat, y, reduction=\"sum\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 15/15 [00:00<00:00, 96.22it/s, loss=2.84e+05, v_num=0] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 15/15 [00:00<00:00, 87.26it/s, loss=2.84e+05, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer()\n",
    "model = LinRegModel(input_dim=13)\n",
    "trainer.fit(model, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kotorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e394b0e5950b2ed9ac02d3d0f43da50df004fa03f183c9ffe2862d4a06a95d21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
